{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7462c43-d1b7-46ea-9c88-06c078c92e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Generator, Iterator, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c49a1-af49-4b82-add5-87e582c3032b",
   "metadata": {},
   "source": [
    "### TorchData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d964d3a6-2a46-4057-a430-343401805f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/dt=2023-01-01/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-02/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-03/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-04/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-05/a300c22cb3554cec95c68957f6ac326f-0.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import FileLister\n",
    "\n",
    "filelist = list(FileLister(\"./data\", recursive=True))\n",
    "filelist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4ef72-d516-4797-a6dd-fb9adfd3cb03",
   "metadata": {},
   "source": [
    "# ParquetDataset\n",
    "\n",
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e31dc0f-528c-4475-ace4-085a6be8547c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas shape : (50000000, 2)\n",
      "Pandas  size : 450000734\n",
      "Pyarrow size : 64\n",
      "files        : ['./data/dt=2023-01-01/a300c22cb3554cec95c68957f6ac326f-0.parquet', './data/dt=2023-01-02/a300c22cb3554cec95c68957f6ac326f-0.parquet', './data/dt=2023-01-03/a300c22cb3554cec95c68957f6ac326f-0.parquet']\n",
      "fragments    : [<pyarrow.dataset.ParquetFileFragment path=./data/dt=2023-01-01/a300c22cb3554cec95c68957f6ac326f-0.parquet partition=[dt=2023-01-01]>, <pyarrow.dataset.ParquetFileFragment path=./data/dt=2023-01-02/a300c22cb3554cec95c68957f6ac326f-0.parquet partition=[dt=2023-01-02]>, <pyarrow.dataset.ParquetFileFragment path=./data/dt=2023-01-03/a300c22cb3554cec95c68957f6ac326f-0.parquet partition=[dt=2023-01-03]>]\n",
      "files rows   : [8640000, 8640000, 8640000, 8640000, 8640000, 6800000]\n",
      "column size  : 2\n"
     ]
    }
   ],
   "source": [
    "from pyarrow.parquet import ParquetDataset\n",
    "\n",
    "dataset = ParquetDataset(\"./data\", memory_map=True, use_legacy_dataset=False)\n",
    "df = pd.read_parquet(\"./data\")\n",
    "\n",
    "file_rows = [frag.count_rows() for frag in dataset.fragments]\n",
    "\n",
    "print(\"Pandas shape :\", df.shape)\n",
    "print(\"Pandas  size :\", sys.getsizeof(df))\n",
    "print(\"Pyarrow size :\", sys.getsizeof(dataset))\n",
    "print(\"files        :\", dataset.files[:3])\n",
    "print(\"fragments    :\", dataset.fragments[:3])\n",
    "print(\"files rows   :\", file_rows)\n",
    "print(\"column size  :\", len(dataset.schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f540049-85df-46d9-9815-4384b1df1156",
   "metadata": {},
   "source": [
    "## Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf791db-87ef-46b5-925a-93828e5f05b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frag size   : 72\n",
      "num rows    : 32768\n",
      "Pandas shape: (32768, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx\n",
       "0    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for frag in dataset.fragments:\n",
    "    for batch in frag.to_batches():\n",
    "        df = batch.to_pandas()\n",
    "        row = batch.take(pa.array([0]))\n",
    "\n",
    "        print(\"frag size   :\", sys.getsizeof(frag))\n",
    "        print(\"num rows    :\", batch.num_rows)\n",
    "        print(\"Pandas shape:\", df.shape)\n",
    "        display(row.to_pandas())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c85fdc-60a9-4970-b325-08fb2b8489d9",
   "metadata": {},
   "source": [
    "## Create Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adac7f0c-3fa1-4b27-8072-dc6e24e724ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    df = pd.DataFrame({\"idx\": range(50000000)})\n",
    "    dt = datetime(2023, 1, 1)\n",
    "    df[\"dt\"] = df[\"idx\"].apply(\n",
    "        lambda x: (dt + timedelta(milliseconds=x * 10)).date()\n",
    "    )\n",
    "    pa.parquet.write_to_dataset(\n",
    "        pa.Table.from_pandas(df),\n",
    "        root_path=\"data\",\n",
    "        partition_cols=[\"dt\"],\n",
    "        use_legacy_dataset=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86634ed3-5b5f-4cdb-9a7b-1a7de80c9962",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d7752-ae4b-4e39-8a8e-9074e627e6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__getitem__ 50000\n",
      "_get_next 01 50000\n",
      "_get_next 02 50000\n",
      "_get_next 03 50000\n",
      "ITER: 0 <= 50000 < 32768 | 266336\n",
      "ITER: 32768 <= 50000 < 65536 | 266336\n",
      "_get_next 04 50000\n",
      "50000\n",
      "__getitem__ 0\n",
      "_get_next 01 0\n",
      "_get_next 02 0\n",
      "_get_next 03 0\n",
      "ITER: 0 <= 0 < 32768 | 266336\n",
      "_get_next 04 0\n",
      "0\n",
      "__getitem__ 500000\n",
      "_get_next 01 500000\n",
      "_get_next 02 500000\n",
      "_get_next 03 500000\n",
      "ITER: 0 <= 500000 < 32768 | 266336\n",
      "ITER: 32768 <= 500000 < 65536 | 266336\n",
      "ITER: 65536 <= 500000 < 98304 | 266336\n",
      "ITER: 98304 <= 500000 < 131072 | 266336\n",
      "ITER: 131072 <= 500000 < 163840 | 266336\n",
      "ITER: 163840 <= 500000 < 196608 | 266336\n",
      "ITER: 196608 <= 500000 < 229376 | 266336\n",
      "ITER: 229376 <= 500000 < 262144 | 266336\n",
      "ITER: 262144 <= 500000 < 294912 | 266336\n",
      "ITER: 294912 <= 500000 < 327680 | 266336\n",
      "ITER: 327680 <= 500000 < 360448 | 266336\n",
      "ITER: 360448 <= 500000 < 393216 | 266336\n",
      "ITER: 393216 <= 500000 < 425984 | 266336\n",
      "ITER: 425984 <= 500000 < 458752 | 266336\n",
      "ITER: 458752 <= 500000 < 491520 | 266336\n",
      "ITER: 491520 <= 500000 < 524288 | 266336\n",
      "_get_next 04 500000\n",
      "500000\n",
      "여기까지\n",
      "Deleted\n",
      "(67402, 82730)\n",
      "(428, 8, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function PyArrowDataset.__del__ at 0x7f57c9cf9560>\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9179/2190492011.py\", line 123, in __del__\n",
      "AttributeError: '_ParquetDatasetV2' object has no attribute 'clear'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tracemalloc\n",
    "from bisect import bisect_right\n",
    "\n",
    "from pyarrow.dataset import ParquetFileFragment\n",
    "from pyarrow.lib import RecordBatch\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "\n",
    "class PyArrowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Restriction\n",
    "     - Don't shuffle in Dataloader. this is for efficiency to precess large dataset.\n",
    "       If you need to shuffle, do it before this custom dataset. (like in SparkSQL)\n",
    "       But the algorithm supports random access.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source: str, seed: int = 123):\n",
    "        self.source = source\n",
    "        self.seed = seed\n",
    "\n",
    "        # Pyarrow\n",
    "        self.dataset = ParquetDataset(source, use_legacy_dataset=False)\n",
    "        self.fragments: List[ParquetFileFragment] = self.dataset.fragments\n",
    "        self._batches: Iterator[RecordBatch] = None\n",
    "        self._batch: Optional[RecordBatch] = None\n",
    "        self._df: pd.DataFrame = None\n",
    "\n",
    "        # Indexing meta information to make search faster\n",
    "        self._cumulative_n_rows: List[int] = []\n",
    "        self._batch_idx: int = 0\n",
    "\n",
    "        # Index\n",
    "        self._fragment_idx = 0\n",
    "\n",
    "        # Initialization\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        random.seed(self.seed)\n",
    "        # random.shuffle(self.fragments)\n",
    "\n",
    "        self._cumulative_n_rows = [frag.count_rows() for frag in self.fragments]\n",
    "        for i in range(1, len(self._cumulative_n_rows)):\n",
    "            self._cumulative_n_rows[i] += self._cumulative_n_rows[i - 1]\n",
    "\n",
    "    def _get_next(self, idx: int) -> Tuple[int, int]:\n",
    "        print('_get_next 01', idx)\n",
    "        def get_prev_cum_frag_size(_fragment_idx):\n",
    "            if _fragment_idx >= 1:\n",
    "                return self._cumulative_n_rows[_fragment_idx - 1]\n",
    "            return 0\n",
    "\n",
    "        # Calculate fragment idx\n",
    "        fragment_idx = self._fragment_idx\n",
    "        fragment_changed = False\n",
    "        _prev_size = get_prev_cum_frag_size(fragment_idx)\n",
    "        _cur_size = self._cumulative_n_rows[self._fragment_idx]\n",
    "        if (idx < _prev_size) or (idx >= _cur_size):\n",
    "            fragment_idx = bisect_right(self._cumulative_n_rows, idx)\n",
    "            assert fragment_idx < len(self.fragments)\n",
    "            # fragment_idx %= len(self.fragments)\n",
    "            fragment_changed = self._fragment_idx != fragment_idx\n",
    "            self._fragment_idx = fragment_idx\n",
    "            self._batch_idx = 0\n",
    "            \n",
    "            if self._batches:\n",
    "                self._batches.clear()\n",
    "                \n",
    "            del self._batches\n",
    "            del self._batch\n",
    "            del self._df\n",
    "            self._batches = None\n",
    "            self._batch = None\n",
    "            self._df = None\n",
    "        \n",
    "        print('_get_next 02', idx)\n",
    "        # Calculate batch idx\n",
    "        _prev_size = get_prev_cum_frag_size(fragment_idx)\n",
    "        batch_idx = idx - _prev_size\n",
    "        batch_changed = batch_idx < self._batch_idx\n",
    "\n",
    "        # Calculate batches of the fragment\n",
    "        if self._batches is None or fragment_changed or batch_changed:\n",
    "            if self._batches:\n",
    "                self._batches.clear()\n",
    "            \n",
    "            self.batches = self.fragments[fragment_idx].to_batches()\n",
    "            self._batch = None\n",
    "\n",
    "        if self._batch is None:\n",
    "            self._batch = next(self.batches)\n",
    "            del self._df\n",
    "            self._df = self._batch.to_pandas()\n",
    "            self._batch_idx = 0\n",
    "        \n",
    "        print('_get_next 03', idx)\n",
    "        need_to_load_data = False\n",
    "        while True:\n",
    "            print(\n",
    "                \"ITER:\",\n",
    "                f\"{self._batch_idx} <= {batch_idx} < {self._batch_idx + self._batch.num_rows} | {sys.getsizeof(self._batch)}\",\n",
    "            )\n",
    "            if (\n",
    "                self._batch_idx\n",
    "                <= batch_idx\n",
    "                < self._batch_idx + self._batch.num_rows\n",
    "            ):\n",
    "                if need_to_load_data:\n",
    "                    self._df = self._batch.to_pandas()\n",
    "                break\n",
    "\n",
    "            need_to_load_data = True\n",
    "            self._batch_idx += self._batch.num_rows\n",
    "            self._batch = next(self.batches)\n",
    "        \n",
    "        print('_get_next 04', idx)\n",
    "        return fragment_idx, batch_idx - self._batch_idx\n",
    "    \n",
    "    def __del__(self):\n",
    "        print('Deleted')\n",
    "        if self.dataset:\n",
    "            self.dataset.clear()\n",
    "        \n",
    "        if self.fragments:\n",
    "            self.fragments.clearn\n",
    "        del self.dataset\n",
    "        del self.fragments\n",
    "        del self._batches\n",
    "        del self._batch\n",
    "        del self._df\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._cumulative_n_rows[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print('__getitem__', idx)\n",
    "        fragment_idx, batch_idx = self._get_next(idx)\n",
    "\n",
    "        row = self._df.iloc[batch_idx][[\"idx\"]]\n",
    "        row = row.fillna(0)\n",
    "        row[\"fragment_idx\"] = fragment_idx\n",
    "        row[\"batch_idx\"] = batch_idx\n",
    "        return row, idx\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "tracemalloc.start()\n",
    "dataset = PyArrowDataset(\"./data\")\n",
    "print(dataset[50000][0].idx)\n",
    "print(dataset[0][0].idx)\n",
    "print(dataset[500000][0].idx)\n",
    "\n",
    "print('여기까지')\n",
    "del dataset\n",
    "print(tracemalloc.get_traced_memory())\n",
    "print(gc.get_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721df80-e665-40b4-b281-5f5ee285406d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "data, labels = next(iter(loader))\n",
    "a = data[:, 0] - 1\n",
    "b = labels % 1000\n",
    "\n",
    "a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b078d63-87a2-45b2-8316-1622e5e1d39d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ParquetFile\n",
    "\n",
    "## Row 갯수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9036712-a6f0-47db-bf95-358bcdcabca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "parquet_file = ParquetFile(\"./data/dt=20230101/userdata.parquet\")\n",
    "\n",
    "print(\"parquet_file size: \", sys.getsizeof(parquet_file))\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a0352e-1049-43cf-9f4b-651c2b0a215f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size : 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./data/dt=20230101/userdata.parquet']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarrow.parquet import ParquetDataset\n",
    "\n",
    "dataset = ParquetDataset(\"./data\")\n",
    "\n",
    "print(\"dataset size :\", sys.getsizeof(dataset))\n",
    "\n",
    "dataset.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eda780-7a45-495a-8a9e-aae88ce0cf49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyEnv 3.7.15",
   "language": "python",
   "name": "3.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
