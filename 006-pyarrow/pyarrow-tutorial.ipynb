{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7462c43-d1b7-46ea-9c88-06c078c92e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Generator, Iterator, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c49a1-af49-4b82-add5-87e582c3032b",
   "metadata": {},
   "source": [
    "### TorchData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d964d3a6-2a46-4057-a430-343401805f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/dt=2023-01-01/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-02/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-03/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-04/a300c22cb3554cec95c68957f6ac326f-0.parquet',\n",
       " './data/dt=2023-01-05/a300c22cb3554cec95c68957f6ac326f-0.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import FileLister\n",
    "\n",
    "filelist = list(FileLister(\"./data\", recursive=True))\n",
    "filelist[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4ef72-d516-4797-a6dd-fb9adfd3cb03",
   "metadata": {},
   "source": [
    "# ParquetDataset\n",
    "\n",
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e31dc0f-528c-4475-ace4-085a6be8547c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas shape : (50000000, 2)\n",
      "Pandas  size : 450000734\n",
      "Pyarrow size : 64\n",
      "files        : ['./data/dt=2023-01-01/a300c22cb3554cec95c68957f6ac326f-0.parquet', './data/dt=2023-01-02/a300c22cb3554cec95c68957f6ac326f-0.parquet', './data/dt=2023-01-03/a300c22cb3554cec95c68957f6ac326f-0.parquet']\n",
      "fragments    : [<pyarrow.dataset.ParquetFileFragment path=./data/dt=2023-01-01/a300c22cb3554cec95c68957f6ac326f-0.parquet partition=[dt=2023-01-01]>, <pyarrow.dataset.ParquetFileFragment path=./data/dt=2023-01-02/a300c22cb3554cec95c68957f6ac326f-0.parquet partition=[dt=2023-01-02]>, <pyarrow.dataset.ParquetFileFragment path=./data/dt=2023-01-03/a300c22cb3554cec95c68957f6ac326f-0.parquet partition=[dt=2023-01-03]>]\n",
      "files rows   : [8640000, 8640000, 8640000, 8640000, 8640000, 6800000]\n",
      "column size  : 2\n"
     ]
    }
   ],
   "source": [
    "from pyarrow.parquet import ParquetDataset, ParquetFile\n",
    "\n",
    "dataset = ParquetDataset(\"./data\", memory_map=True, use_legacy_dataset=False)\n",
    "df = pd.read_parquet(\"./data\")\n",
    "\n",
    "file_rows = [frag.count_rows() for frag in dataset.fragments]\n",
    "\n",
    "print(\"Pandas shape :\", df.shape)\n",
    "print(\"Pandas  size :\", sys.getsizeof(df))\n",
    "print(\"Pyarrow size :\", sys.getsizeof(dataset))\n",
    "print(\"files        :\", dataset.files[:3])\n",
    "print(\"fragments    :\", dataset.fragments[:3])\n",
    "print(\"files rows   :\", file_rows)\n",
    "print(\"column size  :\", len(dataset.schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f540049-85df-46d9-9815-4384b1df1156",
   "metadata": {},
   "source": [
    "## Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf791db-87ef-46b5-925a-93828e5f05b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frag size   : 72\n",
      "num rows    : 32768\n",
      "Pandas shape: (32768, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx\n",
       "0    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for frag in dataset.fragments:\n",
    "    for batch in frag.to_batches():\n",
    "        df = batch.to_pandas()\n",
    "        row = batch.take(pa.array([0]))\n",
    "\n",
    "        print(\"frag size   :\", sys.getsizeof(frag))\n",
    "        print(\"num rows    :\", batch.num_rows)\n",
    "        print(\"Pandas shape:\", df.shape)\n",
    "        display(row.to_pandas())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97089466-c70d-4ec8-8fab-8ebd3d6d5e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 32768, <pyarrow.parquet.core.ParquetFile at 0x7f8e028a5b90>, 0, 32768),\n",
       " (1,\n",
       "  32768,\n",
       "  65536,\n",
       "  <pyarrow.parquet.core.ParquetFile at 0x7f8e028a5b90>,\n",
       "  1,\n",
       "  32768),\n",
       " (2,\n",
       "  65536,\n",
       "  98304,\n",
       "  <pyarrow.parquet.core.ParquetFile at 0x7f8e028a5b90>,\n",
       "  2,\n",
       "  32768),\n",
       " (3,\n",
       "  98304,\n",
       "  131072,\n",
       "  <pyarrow.parquet.core.ParquetFile at 0x7f8e028a5b90>,\n",
       "  3,\n",
       "  32768),\n",
       " (4,\n",
       "  131072,\n",
       "  163840,\n",
       "  <pyarrow.parquet.core.ParquetFile at 0x7f8e028a5b90>,\n",
       "  4,\n",
       "  32768)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "idx = 0\n",
    "parquet_indices = []\n",
    "for frag in dataset.fragments:\n",
    "    parquet_file = ParquetFile(frag.path)\n",
    "    for i, row_group in enumerate(frag.row_groups):\n",
    "        start_idx = idx\n",
    "        end_idx = idx + row_group.num_rows\n",
    "        parquet_indices.append((i, start_idx, end_idx, parquet_file, row_group.id, row_group.num_rows))\n",
    "        idx += row_group.num_rows\n",
    "        \n",
    "parquet_indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3dbd0dc-55c5-41a7-b175-55a210f9036e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28800"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_indices[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ff55871-98ee-4107-bcca-e6d66a519850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "idx: int64\n",
       "----\n",
       "idx: [[43200000,43200001,43200002,43200003,43200004,...,43220987,43220988,43220989,43220990,43220991]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = frag.row_groups[0]\n",
    "group.id\n",
    "\n",
    "\n",
    "pf = ParquetFile(frag.path)\n",
    "table = pf.read_row_group(0)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c85fdc-60a9-4970-b325-08fb2b8489d9",
   "metadata": {},
   "source": [
    "## Create Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac7f0c-3fa1-4b27-8072-dc6e24e724ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    df = pd.DataFrame({\"idx\": range(50000000)})\n",
    "    dt = datetime(2023, 1, 1)\n",
    "    df[\"dt\"] = df[\"idx\"].apply(\n",
    "        lambda x: (dt + timedelta(milliseconds=x * 10)).date()\n",
    "    )\n",
    "    pa.parquet.write_to_dataset(\n",
    "        pa.Table.from_pandas(df),\n",
    "        root_path=\"data\",\n",
    "        partition_cols=[\"dt\"],\n",
    "        use_legacy_dataset=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86634ed3-5b5f-4cdb-9a7b-1a7de80c9962",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4538a8-0733-4862-8e84-205490cb9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyArrowDataset(Dataset):\n",
    "    def __init__(self, source:str, seed:int =123):\n",
    "        pass\n",
    "    \n",
    "    def init_indexing(self, shuffle:bool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d7752-ae4b-4e39-8a8e-9074e627e6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tracemalloc\n",
    "from bisect import bisect_right\n",
    "\n",
    "from pyarrow.dataset import ParquetFileFragment\n",
    "from pyarrow.lib import RecordBatch\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "\n",
    "class PyArrowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Restriction\n",
    "     - Don't shuffle in Dataloader. this is for efficiency to precess large dataset.\n",
    "       If you need to shuffle, do it before this custom dataset. (like in SparkSQL)\n",
    "       But the algorithm supports random access.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source: str, seed: int = 123):\n",
    "        self.source = source\n",
    "        self.seed = seed\n",
    "\n",
    "        # Pyarrow\n",
    "        self.dataset = ParquetDataset(source, use_legacy_dataset=False)\n",
    "        self.fragments: List[ParquetFileFragment] = self.dataset.fragments\n",
    "        self._batches: Iterator[RecordBatch] = None\n",
    "        self._batch: Optional[RecordBatch] = None\n",
    "        self._df: pd.DataFrame = None\n",
    "\n",
    "        # Indexing meta information to make search faster\n",
    "        self._cumulative_n_rows: List[int] = []\n",
    "        self._batch_idx: int = 0\n",
    "\n",
    "        # Index\n",
    "        self._fragment_idx = 0\n",
    "\n",
    "        # Initialization\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        random.seed(self.seed)\n",
    "        # random.shuffle(self.fragments)\n",
    "\n",
    "        self._cumulative_n_rows = [frag.count_rows() for frag in self.fragments]\n",
    "        for i in range(1, len(self._cumulative_n_rows)):\n",
    "            self._cumulative_n_rows[i] += self._cumulative_n_rows[i - 1]\n",
    "\n",
    "    def _get_next(self, idx: int) -> Tuple[int, int]:\n",
    "        print('_get_next 01', idx)\n",
    "        def get_prev_cum_frag_size(_fragment_idx):\n",
    "            if _fragment_idx >= 1:\n",
    "                return self._cumulative_n_rows[_fragment_idx - 1]\n",
    "            return 0\n",
    "\n",
    "        # Calculate fragment idx\n",
    "        fragment_idx = self._fragment_idx\n",
    "        fragment_changed = False\n",
    "        _prev_size = get_prev_cum_frag_size(fragment_idx)\n",
    "        _cur_size = self._cumulative_n_rows[self._fragment_idx]\n",
    "        if (idx < _prev_size) or (idx >= _cur_size):\n",
    "            fragment_idx = bisect_right(self._cumulative_n_rows, idx)\n",
    "            assert fragment_idx < len(self.fragments)\n",
    "            # fragment_idx %= len(self.fragments)\n",
    "            fragment_changed = self._fragment_idx != fragment_idx\n",
    "            self._fragment_idx = fragment_idx\n",
    "            self._batch_idx = 0\n",
    "            \n",
    "            if self._batches:\n",
    "                self._batches.clear()\n",
    "                \n",
    "            del self._batches\n",
    "            del self._batch\n",
    "            del self._df\n",
    "            self._batches = None\n",
    "            self._batch = None\n",
    "            self._df = None\n",
    "        \n",
    "        print('_get_next 02', idx)\n",
    "        # Calculate batch idx\n",
    "        _prev_size = get_prev_cum_frag_size(fragment_idx)\n",
    "        batch_idx = idx - _prev_size\n",
    "        batch_changed = batch_idx < self._batch_idx\n",
    "\n",
    "        # Calculate batches of the fragment\n",
    "        if self._batches is None or fragment_changed or batch_changed:\n",
    "            if self._batches:\n",
    "                self._batches.clear()\n",
    "            \n",
    "            self.batches = self.fragments[fragment_idx].to_batches()\n",
    "            self._batch = None\n",
    "\n",
    "        if self._batch is None:\n",
    "            self._batch = next(self.batches)\n",
    "            del self._df\n",
    "            self._df = self._batch.to_pandas()\n",
    "            self._batch_idx = 0\n",
    "        \n",
    "        print('_get_next 03', idx)\n",
    "        need_to_load_data = False\n",
    "        while True:\n",
    "            print(\n",
    "                \"ITER:\",\n",
    "                f\"{self._batch_idx} <= {batch_idx} < {self._batch_idx + self._batch.num_rows} | {sys.getsizeof(self._batch)}\",\n",
    "            )\n",
    "            if (\n",
    "                self._batch_idx\n",
    "                <= batch_idx\n",
    "                < self._batch_idx + self._batch.num_rows\n",
    "            ):\n",
    "                if need_to_load_data:\n",
    "                    self._df = self._batch.to_pandas()\n",
    "                break\n",
    "\n",
    "            need_to_load_data = True\n",
    "            self._batch_idx += self._batch.num_rows\n",
    "            self._batch = next(self.batches)\n",
    "        \n",
    "        print('_get_next 04', idx)\n",
    "        return fragment_idx, batch_idx - self._batch_idx\n",
    "    \n",
    "    def __del__(self):\n",
    "        print('Deleted')\n",
    "        if self.dataset:\n",
    "            self.dataset.clear()\n",
    "        \n",
    "        if self.fragments:\n",
    "            self.fragments.clearn\n",
    "        del self.dataset\n",
    "        del self.fragments\n",
    "        del self._batches\n",
    "        del self._batch\n",
    "        del self._df\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._cumulative_n_rows[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print('__getitem__', idx)\n",
    "        fragment_idx, batch_idx = self._get_next(idx)\n",
    "\n",
    "        row = self._df.iloc[batch_idx][[\"idx\"]]\n",
    "        row = row.fillna(0)\n",
    "        row[\"fragment_idx\"] = fragment_idx\n",
    "        row[\"batch_idx\"] = batch_idx\n",
    "        return row, idx\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "tracemalloc.start()\n",
    "dataset = PyArrowDataset(\"./data\")\n",
    "print(dataset[50000][0].idx)\n",
    "print(dataset[0][0].idx)\n",
    "print(dataset[500000][0].idx)\n",
    "\n",
    "print('여기까지')\n",
    "del dataset\n",
    "print(tracemalloc.get_traced_memory())\n",
    "print(gc.get_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721df80-e665-40b4-b281-5f5ee285406d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "data, labels = next(iter(loader))\n",
    "a = data[:, 0] - 1\n",
    "b = labels % 1000\n",
    "\n",
    "a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b078d63-87a2-45b2-8316-1622e5e1d39d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ParquetFile\n",
    "\n",
    "## Row 갯수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9036712-a6f0-47db-bf95-358bcdcabca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "parquet_file = ParquetFile(\"./data/dt=20230101/userdata.parquet\")\n",
    "\n",
    "print(\"parquet_file size: \", sys.getsizeof(parquet_file))\n",
    "parquet_file.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a0352e-1049-43cf-9f4b-651c2b0a215f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size : 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./data/dt=20230101/userdata.parquet']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarrow.parquet import ParquetDataset\n",
    "\n",
    "dataset = ParquetDataset(\"./data\")\n",
    "\n",
    "print(\"dataset size :\", sys.getsizeof(dataset))\n",
    "\n",
    "dataset.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eda780-7a45-495a-8a9e-aae88ce0cf49",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyEnv 3.7.15",
   "language": "python",
   "name": "3.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
